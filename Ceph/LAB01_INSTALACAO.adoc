= LAB01 - Instalação do Ceph







Seja bem vindo ao Workshop Ceph dedicado a versão 3.2

== Conjunto de recursos do workshop

Este workshop simula um ambiente de produção contando com 9 servidores dedicados para depoloyment e uso do Ceph.

.Informações Blueprint utilizado
|===
|Nome do blueprint| versão | Detalhes
|LATAM-SA-BR-CEPH-LABS-Deploy-V3.3-Completo-bp | 3.3 | Versão do Ceph 3.2 - systemd
|===

.Mapa dos servidores
|===
|IP interno| Servidor | Função
|192.168.10.6  | idm.labs.corp | Servidor de horário , DNS e diretório
|192.168.10.50  | cephdeploy.labs.corp | Servidor para deployment do Ceph
|192.168.10.101 | ceph01.labs.corp | Servidor dedicado para as funções de MON - MGR - MDS
|192.168.10.102 | ceph02.labs.corp | Servidor dedicado para as funções de - MGR - MDS
|192.168.10.103 | ceph03.labs.corp | Servidor dedicado as funções de - MGR - MDS
|192.168.10.104 | ceph04.labs.corp | Servidor dedicado para função OSD
|192.168.10.105 | ceph05.labs.corp | Servidor dedicado para função OSD
|192.168.10.106 | ceph06.labs.corp | Servidor dedicado para função OSD
|192.168.10.107 | ceph07.labs.corp | Ceph  Gateway iSCSI - FileServer
|192.168.10.108 | ceph08.labs.corp | Ceph  Gateway object store
|192.168.10.52  | cephclient.labs.corp | Ceph cliente (node)
|===

.Recursos de rede
|===
|Recurso |IP
|NTP     |192.168.10.6
|DNS     |192.168.10.6
|GATEWAY |192.168.10.50
|===


.Mapa dos usuários utilizados no workshop
|===
|Usuários |Senha| Função
|root    |rooter2019 | usuário root
|ansible |rooter2019 | usuário ansible (usando para gerência e deployment)
|admin   |rooter2019 | usuário administrativo do IDM
|===

== O que é necessário para acessar o ambiente do laborátorio ?

* Acesso a Internet
* Cliente SSH
  - Windows Donwload Putty SSH: https://the.earth.li/~sgtatham/putty/latest/w64/putty-64bit-0.71-installer.msi
  - Passo para conectar a um servidor Linux usando Putty SSH: https://www.ssh.com/ssh/putty/windows/

=== Workout do laborátorio01 - Atividades

.Atividades do laboratório01
|===
|Atividade | Objetivos
|01| Planejar e instalação do Ceph
|02| Realizar o deployment de um Cluster Ceph utilizando ansible
|03| Executar o  deployment Ceph Ansible
|04| Checar a saúde do cluster pós instalação
|===


=== Atividade 01: _Planejando a instalação do Ceph_

==== Validando a configuração do Ceph Storage

==== Requisitos de hardware

Trabalhando com requisitos mínimo de hardware. Não há uma única opção de hardware recomendada para implantar o Red Hat Ceph Storage. Dependendo do caso de uso suportado pelo cluster, seja otimização de IOPS, taxa de transferência ou capacidade, uma configuração de hardware diferente pode ser aplicada. Os requisitos para cada caso de uso podem se concentrar em discos de alto desempenho (IOPS), especificações gerais de throughput de hardware (taxa de transferência) ou unidades de disco (capacidade) mais baratas.

É possível executar um cluster focado prova de conceito com hardware com baixo potencial. Para uma implementação de produção, no entanto, os requisitos mínimos de hardware recomendados para cada daemon Ceph estão listados abaixo:

.Hardware recomendado mínimo para Ceph Daemons
|===
|Hardware | ceph-osd | ceph-mon | ceph-radosgw | ceph-mdss
|Processor| 1x AMD64 or Intel64 | 1x ADM64 or Intel 64| 1x AMD64 or Intel 64 | 1x AMD64 or Intel 64
|RAM| 16 GB por host, mais 2 GB adicional por OSD daemon 	 | 1 GB por daemon | 1 GB por daemon | 1 GB por daemon
|Disk| Um storage dispositivo por OSD daemon. Separar do sistema do sistema operacional | 10 GB por daemon | 5 GB por daemon | 1 MB por daemon, mais espaço para logs
|Network| 2x Gigabit Ehternet NICs | 2x Gigabit Ethernet NICs | 2x Gigabit Ethernet NICs | 2x Gigabit Ethernet NICs
|===

Pontos importantes.

 * Ao determinar quantos OSDs fornecer de cada host OSD, tenha cuidado ao selecionar uma densidade de armazenamento balanceada (o número de OSDs fornecidos por host OSD). Um cluster de armazenamento menos denso implica que os daemons e processos do Ceph são distribuídos por mais hosts, distribuindo a carga de trabalho. Um erro arquitetural comum é construir um pequeno cluster com uma densidade de armazenamento muito alta, levando a tráfego de rede excessivo durante as operações de rebalanceamento e recuperação.
 * Sobre o uso de RAID. Geralmente, os dispositivos usados pelos OSDs não devem usar o RAID. O Ceph usa a codificação de replicação ou eliminação para proteger objetos no cluster, portanto, o uso de RAID para proteção adicional de dados para OSDs é normalmente desnecessário, adiciona custo e reduz a capacidade.

==== Recomendação de rede

Recomendamos que, se possível, 10 Gigabit Ethernet seja usado para produção. Hosts OSD densos devem usar um link Ethernet de 10 Gigabits para cada 12 OSDs fornecidos pelo host OSD. Uma rede mais rápida permitirá que o cluster se reequilibre e recupere mais rapidamente em caso de falha do OSD.

Além disso, a configuração padrão coloca todo o tráfego em uma rede. É melhor configurar redes separadas para tráfego público e de cluster. A rede pública é usada para o tráfego do cliente e comunicação com o MONs. A rede de cluster é usada para pulsação OSD, replicação, backfilling e tráfego de recuperação. Essas redes devem usar placas de interface de rede física separadas.

Pontos que precisam ser checados na parte de rede antes da instalação

* Endereço IP estático
* Ativação da configuração de rede no boot
* NTP sincronizado em cada nó participante do cluster
* Configuração Firewall

.Configuração do Firewall
|===
|Nome do serviço | Portas | Descrição
|Monitor | 6789/TCP | Comunicação do Ceph cluster
|Manager | 7000/TCP - 8003/TCP - 9283/TCP | Comunicação Ceph Manager dasboard - Ceph Manager RESTful API via HTTPS - Comunicação Prometheus plug-in
|OSD | 6800-7300/TCP | Cada OSD utiliza tres dentro deste range. Um porta para comunicação com clientes e monitoramento sobre a rede pública;uma porta para enviar dados para outros nós OSD ou sobre rede pública;e terceira é para troca de pacotes heartbeat na rede cluster ou publica
|RADOS Gateway| 7480/TCP | RADOS Gateway utiliza porta 7480/TCP mas está porta pode ser alterada para porta 80/TCP ou 443/TCO se estiver usando SSL/TLS
|===

==== Validando a configuração de pré-requisitos de software - _Red Enterprise Linux Server_

Antes de iniciar a instalação de um cluster Ceph é necessário preparr o sistema operacional indepedente da função que o nó irá desempenhar dentro do cluster Ceph

Pontos que precisam ser checados antes da instalação

* Realize uma instalação básica da mesma versão do Red Hat Enterprise Linux 7 em todos os hosts.

* Use o comando subscription-manager para registrar os sistemas e ativar os canais de software corretos (ou ter as imagens ISO disponíveis para uma instalação desconectada).

* Configure a resolução de rede e nome para todos os hosts e configure a sincronização de horário do NTP.

* Garanta a configuração correta do firewall local.

* Configure um usuário (não ceph) em todos os nós para uso pelo Ansible e garanta o acesso do sudo ao root.

* Certifique-se de que o usuário que executará os Ansible Playbooks no nó de implementação possa usar a autenticação baseada em chave SSH para efetuar login como o usuário Ansible nos nós do cluster.

* Verifique se o nó de implementação pode executar tarefas Ansible nos nós do cluster.


==== Repositórios - _Red Enterprise Linux Server_

Os repositórios para instalação do Ceph devem ser subscritos em todos os servidores participantes do cluster do Ceph.

.Procedimentos para subcrição dos servidores Ceph
|===
|Passo | Comando | Descrição
|1| subscription-manager --disable="*"  | Desabilita todos os repositórios. É necessário para que outros reposótirios afetem a instalação do Ceph
|2| subscription-manager --enable=rhel-7-server-rpms --enable=rhel-7-server-extras-rpms| Repositórios básicos do RHEL para instalação do Ceph. Deve estar assinado em todos nós participantes do cluster
|3| subscription-manager --enable=rhel-7-server-rhceph-3-mon-rpms | Subscreva todos os nós responsáveis pelo  monitoramento
|4| subscription-manager --enable=rhel-7-server-rhceph-3-osd-rpms | Subscreva todos os nós responsáveis pelo  pelo OSD
|5| subscription-manager --enable=rhel-7-server-rhceph-3-tools-rpms | Subscreva todos os nós responsáveis pelo  - Ansible deployment host, MDS, RADOS Gateway, ou Ceph client
|===

==== Preparando o usuário Ansible

O Red Hat Ceph Storage fornece um conjunto de Ansible Playbooks para instalar e configurar seu cluster Ceph. Você executará um playbook como um usuário comum no host de implementação e o Ansible efetuará login nos hosts do cluster para instalá-los e configurá-los.

Antes de usar esses playbooks, você precisa preparar o host de implantação e os hosts de cluster com um usuário comum para Ansible.

Esse usuário no host de implementação é configurado para efetuar login como o mesmo usuário em qualquer um dos hosts de cluster usando a autenticação baseada em chave SSH. O usuário também é configurado em todos os hosts para ter acesso sudo sem senha para executar comandos como o usuário root.

O procedimento básico para configurar isso é:

* Crie um usuário para Ansible que tenha o mesmo nome de usuário no host de implementação e em todos os hosts de cluster. Para esta discussão, usamos o nome ansible, mas qualquer nome não utilizado funcionará. Não use o nome de usuário ceph, que é reservado para os daemons do Ceph.

* Use ssh-keygen para gerar um par de chaves SSH na conta do usuário no host de implementação.

* Distribuir a chave pública SSH do usuário ansiável para o arquivo ~ / .ssh / authorized_keys do usuário em todos os hosts. Certifique-se de que o arquivo de chaves autorizadas seja legível apenas pelo usuário.

* Crie um arquivo /etc/sudoers.d/ansible que conceda o acesso root sem senha do usuário ansible. Ele deve ser legível apenas pelo usuário root e ter um conteúdo semelhante ao seguinte:

  ansible ALL=(root) NOPASSWD:ALL


=== Atividade 02: _Realizando deployment do Cluster Ceph usando Ansible_

==== Instalação do deployment Red Hat Ceph Ansible

Quando todos os servidores participantes do cluster Ceph estiverem prontos. É necessário instalar o pacote _ceph-ansible_ em seu servidor de deployment

 #yum install -y ceph-ansible

===== Sobre o deploymento Ceph-ansible playbooks

Ansible é um utilitário de automação, gerenciamento de configuração e orquestração de código aberto. Pode automatizar e padronizar a configuração de servidores remotos e máquinas virtuais.

O Red Hat Ceph Storage usa o Ansible para instalar e configurar o cluster do Ceph.

Ansible automatiza as operações através de um playbook baseado em texto. Um playbook é um arquivo que contém uma lista de reproduções que precisam ser executadas em uma lista especificada de sistemas.

Cada PLAYBOOK é composto por uma série de tarefas para executar em sequência. O Ansible é projetado para que os playbooks bem escritos sejam idempotentes, o que significa que eles podem ser executados várias vezes e alterar apenas as coisas que precisam ser alteradas nos hosts de destino.

Isso permite que um manual seja executado novamente para corrigir erros ou alterações que foram feitas nos sistemas de destino.

Para determinar a lista de hosts disponíveis, o Ansible geralmente usa um arquivo de inventário baseado em texto. Um arquivo de inventário muito simples pode consistir em uma lista de hosts, um host por linha.

Um arquivo de inventário mais complexo pode organizar esses hosts em grupos, usando uma sintaxe semelhante a INI, em que cada grupo começa com o nome do grupo entre colchetes, seguido pela lista de hosts no grupo. Um host pode ser listado em vários grupos. O grupo especial inclui todos os hosts no inventário.


==== Entendendo  arquivos de configuração do Ceph Ansible

Para realizar o deploymento do Red Hat Ceph Storage 3 usando cluster é necessário entender o papel dos seus principais playbooks.

.Inventário dos playbooks Ansible_
|===
|Arquivo de configuração | Funcionalidade | exemplo
|/etc/ansible/hosts | Configure um arquivo de inventário Ansible para listar seus nós de cluster e atribuí-los aos grupos com base em suas funções (OSD, MON, cliente e assim por diante). | colocar public_network
|/usr/share/ceph-ansible/group_vars/all.yml | Grupo de arquivo  variável de grupo com diretivas que se aplicam a todos os hosts do Ceph.
|/usr/share/ceph-ansible/group_vars/osds.yml | Grupo de arquivo variável de grupo com diretivas que se aplicam aos OSDs.
|/usr/share/ceph-ansible/group_vars/clients.yml | Grupo arquivo de variável de grupo com diretivas que se aplicam a clientes.
|/usr/share/ceph-ansible/site.yml | Arquivo  responsável pela start da instalação
|===


==== Configuração do inventário

Seu primeiro passo é configurar seu arquivo de inventário. Os playbooks executáveis usam um grupo de hosts para cada tipo de nó Ceph: mons para monitores, osds para OSDs, mgrs para gerenciadores, mdss para MDSs, clientes para clientes Ceph, rgws para nós Gateway RADOS, iscsi-gws para gateways iSCSI e rbd-espelhos para ativar o espelhamento de RBD.

Você precisa atribuir seus hosts Ceph aos grupos apropriados no arquivo de inventário com base em sua função.

*Exemplo do arquivo de configuração do inventário*
 [root@server ~]# cat /etc/ansible/hosts
 [mons]
 monitor-host-name

 [mgrs]
 manager-host-name

 [osds]
 osd-host-name

==== Ajustando ceph-ansible playbook

*Dica para visualização e edição do arquivo YAML (Ansible)*

Se você usar o editor de texto Vim, poderá aplicar algumas configurações que facilitarão a edição de seus playbooks e arquivos variáveis. Por exemplo, adicione a seguinte linha ao seu arquivo $ HOME / .vimrc, e se o vim detectar que você está editando um arquivo YAML, ele recua por dois espaços quando você pressiona a tecla Tab, identifica as linhas subsequentes e expande as guias nos espaços.

 autocmd FileType yaml setlocal ai ts=2 sw=2 et


===== Configurando o group_vars/all.yml

O arquivo group_vars / all.yml contém parâmetros comuns que são aplicados a todos os nós do Ceph no inventário.

A tabela a seguir lista alguns parâmetros principais que são comumente definidos.

|===
|Nome do parametro | Valor Descrição | Descrição
| fetch_directory| ~/ceph-ansible-keys | Localização do diretório temporário usado para copiar chaves de autenticação para nós do cluster
| ceph_origin | repository | Onde obter Ceph de. repositório usa um repositório de pacotes
| ceph_repository | rhcs | O repositório usado para instalar o Red Hat Ceph Storage. O rhcs usa pacotes oficiais do Red Hat Ceph Storage
| ceph_repository_type | cdn or iso | A fonte de instalação para rhcs, a Content Distribution Network ou uma imagem ISO local
| ceph_rhcs_iso_path |Path to ISO | Se estiver usando o tipo de repositório iso, o caminho para o ISO de Armazenamento do Red Hat Ceph
| ceph_rhcs_version |"3" |A versão do Red Hat Ceph Storage para instalar
| monitor_interface |Network Interface |A interface de rede na qual o monitor escuta
| public_network | Address and Netmask | A sub-rede da rede pública do cluster, por exemplo, 192.168.122.0/24
| cluster_network |Address and Netmask | A sub-rede da rede privada do cluster. Padrões para o valor definido para o public_network
| journal_size | Size in MB | O tamanho a ser alocado para periódicos OSD. Deve ser o dobro do rendimento esperado durante o intervalo máximo de sincronização do armazenamento de arquivos (que é cinco segundos por padrão). Não deve ser menor que 5120 MB na maioria dos casos.
|===

====== Exemplo simples do *group_vars/all.yml*

 fetch_directory: ~/ceph-ansible-keys
 ceph_origin: repository
 ceph_repository: rhcs
 ceph_repository_type: cdn
 ceph_rhcs_version: "3"
 monitor_interface: eth0
 public_network: 192.168.122.0/24
 cluster_network: "{{ public_network }}"

 journal_size: 5120

====== Parametro Overrides
O parâmetro ceph_conf_overrides dentro do arquivo group_vars / all.yml permite substituir os valores padrão de configuração do Ceph para um determinado parâmetro dentro de uma seção. Talvez seja necessário usar isso para ajustar ou configurar corretamente seu cluster.

Por exemplo, se você precisar alterar o valor do parâmetro ceph_parameter_name de uma seção, inclua a seguinte diretiva em group_vars / all.yml:

 ceph_conf_overrides:
   section_name:
     ceph_parameter_name: value

===== Configurando o group_vars/osds

 As opções de implementação do OSD são definidas no arquivo de configuração /usr/share/ceph-ansible/group_vars/osds.yml.
 As variáveis configuradas nesse arquivo se aplicam a todos os hosts no grupo osds no inventário.
 No mínimo, você precisa definir a lista de dispositivos a serem usados como OSDs e se os dados do OSD e as informações do diário devem ser colocados no mesmo dispositivo ou se eles usam dispositivos separados.

Você vai se concentrar no cenário colocado no momento e assumir que todos os hosts OSD possuem hardware idêntico e usam os mesmos nomes de dispositivo para seus OSDs.
Para este caso, um arquivo básico group_vars / osds.yml deve conter duas diretivas: uma variável osd_scenario definida como "collocated" e uma variável de dispositivos definida como uma lista de dispositivos a serem usados como OSDs.

O seguinte arquivo group_vars / osds.yml configura três dispositivos em cada host OSD como OSDs:

 osd_scenario: "collocated"
 devices:
   - /dev/sdb
   - /dev/sdc


Como alternativa, você pode configurar seus hosts OSD usando o cenário "não-colocado".
Isso usa dispositivos de armazenamento separados para dados OSD e o diário OSD. Um exemplo do arquivo group_vars / osds.yml aparece abaixo:

 osd_scenario: "non-collocated"
 devices:
   - /dev/sdb
   - /dev/sdc
 dedicated_devices:
   - /dev/sdd
   - /dev/sde

Neste exemplo, você tem dois OSDs em cada host OSD: / dev / sdb armazena dados e usa / dev / sdd como um diário, e / dev / sdc armazena dados e usa / dev / sde como um diário.

Parametros chave para group_vars/osds.yml

|===
|Nome do parametro | Valor | Descriçào
|osd_scenario | collocated ou non-collocated | Tipo de implantação de diário do OSD
|devices| Uma lista dos nomes dos dispositivos a serem usados nos OSDs | Dispositivos para dados OSD colocados e partições de diário ou partições de dados OSD não colocados
|dedicated_devices| Uma lista dos nomes dos dispositivos a serem usados em periódicos OSD não colocados | Dispositivos dedicados para OSD journals
|===

Se necessário, use o Guia de Instalação do Red Hat Ceph Storage 3.0 para obter informações sobre outros parâmetros pertinentes, incluindo configurações que usam dispositivos separados para dados OSD e periódicos.
O arquivo */usr/share/ceph-ansible/group_vars/osds.yml.sample* também possui alguns exemplos adicionais.

=== Atividade 03 : _Executar o  deployment Ceph Ansible_

*Ao executar o procedimento de deployment do Cluster Ceph via Ansible tenha certeza dos seguintes pontos:*

1. Faça o login com usuário Ansible  no servidor *cephdeploy.labs.corp* e construa seu  inventario para instalação do Ceph - Utilize como exemplo o arquivo */etc/ansible/inventario-ceph.ini*
2. Tenha certeza de estar dentro do diretorio */usr/share/ceph-ansible*
3. Logado com usuário 'ansible', valide se o usuário ansible está sendo responsável pela conexão nos servidores remotos.

 ansible -i /etc/ansible/inventario-ceph.ini all -m shell -a "id"
 ceph08.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 cephallnode.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph03.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph02.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph01.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph07.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph05.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph04.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 idm.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 cephdeploy.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 ceph06.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

 cephclient.labs.corp | SUCCESS | rc=0 >>
 uid=1001(ansible) gid=1001(ansible) groups=1001(ansible) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

4. Verifique se todos os servidores estão respondendo

   [ansible@cephdeploy ~]$ ansible -i /etc/ansible/inventario-ceph.ini all -m ping
   cephallnode.labs.corp | SUCCESS => {
   "changed": false,
    "ping": "pong"
   }
   ceph08.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph03.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph01.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph04.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph07.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph05.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   cephdeploy.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph06.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   idm.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   cephclient.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }
   ceph02.labs.corp | SUCCESS => {
    "changed": false,
    "ping": "pong"
   }

5. Sincronize o  horário de todos os servidores do grupo *ceph* com o servidor IDM local.

 [ansible@cephdeploy ~]$ ansible -i /etc/ansible/inventario-ceph.ini ceph  -m shell -s -a "ntpdate 192.168.10.6"
 [DEPRECATION WARNING]: The sudo command line option has been deprecated in favor of the "become" command line arguments. This feature will be removed in version 2.6. Deprecation
 warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.
 ceph07.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:27 ntpdate[11683]: adjust time server 192.168.10.6 offset 0.035483 sec

 ceph01.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:27 ntpdate[11788]: adjust time server 192.168.10.6 offset 0.031867 sec

 ceph06.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:28 ntpdate[11927]: adjust time server 192.168.10.6 offset 0.033831 sec

 ceph04.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:28 ntpdate[11852]: adjust time server 192.168.10.6 offset 0.033193 sec

 ceph05.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:28 ntpdate[11855]: adjust time server 192.168.10.6 offset 0.032992 sec

 cephclient.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:34 ntpdate[11623]: adjust time server 192.168.10.6 offset 0.027578 sec

 ceph08.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:34 ntpdate[11624]: adjust time server 192.168.10.6 offset 0.031249 sec

 ceph02.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:35 ntpdate[11793]: adjust time server 192.168.10.6 offset 0.025973 sec

 ceph03.labs.corp | SUCCESS | rc=0 >>
 11 Jun 14:28:35 ntpdate[11782]: adjust time server 192.168.10.6 offset 0.024477 sec


Até este ponto se tudo estiver ok, os seguintes já foram validados

 * Usuário está corretamente configurado
 * Todos os nós foram sincronizados com servidor de horário

Agora iremos realizar  o deployment componente por componente do Ceph. Seguindo a ordem:
  1 - Monitor
  2 - MGRS
  3 - OSD
  4 - Client

=== Atividade 3.1 :  Utilizando Ceph-ansible para instalação do  MON

1. Com usuário altere a permissão da estrutura de diretorio */usr/share/ceph-ansible* para o usuário *ansible*

  sudo chown -R ansible:ansible /usr/share/ceph-ansible

2. Altere o arquivo mons.yml.sample para mons.yml

  cp /usr/share/ceph-ansible/group_vars/mons.yml.sample  /usr/share/ceph-ansible/group_vars/mons.yml

3. Não existe nenhum alteração fora do padrão para MONITOR. Desta podemos deixar o arquivo sem alteração do seu conteudo.

4. Com o arquivo mons.yml configurado, vamos configurar o arquivo de configuração ao cluster.ceph - */usr/share/ceph-ansible/group_vars/all.yml.sample*

5. Altere o arquivo all.yml.sample para all.yml

  cp /usr/share/ceph-ansible/group_vars/all.yml.sample  /usr/share/ceph-ansible/group_vars/all.yml

6. Altere os seguintes parametros do all.yml - *primeira parte*

  fetch_directory: ~/ceph-ansible-serverf-keys
  ntp_service_enabled: false
  ceph_origin: repository
  ceph_repository: rhcs
  ceph_rhcs_version: "3"
  ceph_repository_type: cdn

  rbd_cache: "true"
  rbd_cache_writethrough_until_flush: "false"
  rbd_client_directories: false

  monitor_interface: eth0 <1>

  journal_size: 14000 <2>
  public_network: 192.168.10.0/24 <3>
  cluster_network: "{{ public_network }}"

  common_single_host_mode: false <4>

<1> O monitor_interface é eth0 e as redes pública e de cluster são ambas 192.168.10.0/24.

<2> O parametro  journal_size é para ser  1024 MB.

<3> Estamos utilizando apenas um interface para rede publica e rede privada

<4> common_single_host_mode deve ser definido como true. Este é um modo especial para clusters de nó único.


[%hardbreaks]

Continue alterando o arquivo 'all.yml' e agora pule para o parametro *ceph_conf_overrides* . Deixe o arquivo desta forma

 ceph_conf_overrides:
   global:
     mon_osd_allow_primary_affinity: 1
     mon_clock_drift_allowed: 0.5
     osd_pool_default_size: 2
     osd_pool_default_min_size: 1
     mon_pg_warn_min_per_osd: 0
     mon_pg_warn_max_per_osd: 0
     mon_pg_warn_max_object_skew: 0
   client:
     rbd_default_features: 1

Execelente Trabalho !!! Agora salve e feche o arquivo.
Vamos trabalhar no últiomo arquivo para realizar o deployment do Monitor


Altere o arquivo site.yml . Este arquivo está em /usr/share/ceph-ansible/ mas antes lembre-se de fazer uma cópia de segurança do arquivo

 cp /usr/share/ceph-ansible/site.yml.sample /usr/share/ceph-ansible/site.yml

A intenção neste workshop não é executar a instalação do Ceph de uma vez e sim mostrar o passo a passo do deployment de cada componente do Ceph. Para isto modifique o arquivo site.yml para seguinte forma:

-------
  - hosts:
   - mons
  #  - agents
  #  - osds
  #  - mdss
  #  - rgws
  #  - nfss
  #  - restapis
  #  - rbdmirrors
  #  - clients
  #  - mgrs
  #  - iscsigws
  #  - iscsi-gws # for backward compatibility only!
-------

*Antes de iniciar o deployment check os itens:*

* Tenha certeza que você está dentro do diretório  _/usr/share/ceph-ansible/_
* Entre no seu inventario e comente todos os grupos exceto MONS

*Execute a chamada para iniciar o deployment*

 ansible-playbook -i /etc/ansible/inventario-ceph.ini -l mons site.yml

Ao terminar a instalação do MON você terá uma tela com a seguite saída:

------

ASK [show ceph status for cluster ceph] *****************************************************************************************************************************************
Tuesday 11 June 2019  17:56:46 -0300 (0:00:00.693)       0:05:27.765 **********
ok: [ceph01.labs.corp -> ceph01.labs.corp] => {
    "msg": [
        "  cluster:",
        "    id:     4c595cd7-220d-48d7-80fc-88f4b6dd8a84", <5>
        "    health: HEALTH_OK", <4>
        " ",
        "  services:",
        "    mon: 3 daemons, quorum ceph01,ceph02,ceph03", <1>
        "    mgr: no daemons active",
        "    osd: 0 osds: 0 up, 0 in",
        " ",
        "  data:",
        "    pools:   0 pools, 0 pgs",
        "    objects: 0 objects, 0B",
        "    usage:   0B used, 0B / 0B avail",
        "    pgs:     ",
        " "
    ]
}

PLAY RECAP ***********************************************************************************************************************************************************************
ceph01.labs.corp           : ok=99   changed=8    unreachable=0    failed=0
ceph02.labs.corp           : ok=88   changed=8    unreachable=0    failed=0
ceph03.labs.corp           : ok=89   changed=9    unreachable=0    failed=0
<2>

INSTALLER STATUS *****************************************************************************************************************************************************************
Install Ceph Monitor        : Complete (0:05:01)

Tuesday 11 June 2019  17:56:46 -0300 (0:00:00.067)       0:05:27.832 **********
===============================================================================
ceph-common : install redhat ceph packages ------------------------------------------------------------------------------------------------------------------------------ 205.85s <3>
ceph-common : install redhat dependencies -------------------------------------------------------------------------------------------------------------------------------- 18.86s
ceph-config : generate ceph configuration file: ceph.conf ----------------------------------------------------------------------------------------------------------------- 9.11s
ceph-mon : collect admin and bootstrap keys ------------------------------------------------------------------------------------------------------------------------------- 7.26s
ceph-common : purge yum cache --------------------------------------------------------------------------------------------------------------------------------------------- 5.97s
ceph-config : create ceph initial directories ----------------------------------------------------------------------------------------------------------------------------- 3.65s
ceph-common : check if the red hat storage monitor repo is already present ------------------------------------------------------------------------------------------------ 3.60s
gather and delegate facts ------------------------------------------------------------------------------------------------------------------------------------------------- 2.63s
ceph-infra : open monitor and manager ports ------------------------------------------------------------------------------------------------------------------------------- 1.82s
ceph-mon : copy keys to the ansible server -------------------------------------------------------------------------------------------------------------------------------- 1.70s
ceph-mon : start the monitor service -------------------------------------------------------------------------------------------------------------------------------------- 1.08s
ceph-infra : start firewalld ---------------------------------------------------------------------------------------------------------------------------------------------- 1.00s
ceph-handler : copy mon restart script ------------------------------------------------------------------------------------------------------------------------------------ 0.98s
ceph-facts : create a local fetch directory if it does not exist ---------------------------------------------------------------------------------------------------------- 0.84s
ceph-common : get ceph version -------------------------------------------------------------------------------------------------------------------------------------------- 0.82s
ceph-mon : create monitor initial keyring --------------------------------------------------------------------------------------------------------------------------------- 0.79s
ceph-facts : create a local fetch directory if it does not exist ---------------------------------------------------------------------------------------------------------- 0.72s
ceph-facts : read cluster fsid if it already exists ----------------------------------------------------------------------------------------------------------------------- 0.72s
ceph-facts : read cluster fsid if it already exists ----------------------------------------------------------------------------------------------------------------------- 0.71s
ceph-mon : read monitor initial keyring if it already exists -------------------------------------------------------------------------------------------------------------- 0.71s
[ansible@cephdeploy ceph-ansible]$
------



<1> Repare que apenas o serviço MON foi instalado e sobre os três servidores Ceph01, Ceph02 e Ceph03
<2> O Script trabalhou apenas nos servidores do grupo MON
<3> Tempo total da instalação
<4> Saúde do cluster - Ele está ok.
<5> ID do Cluster

[%hardbreaks]


=== Atividade 3.2 : Utilizando Ceph-ansible para instalação do  MGRS

Já instalamos o MON agora vamos instalar o componente 'MGR'.

1. Altere o arquivo mgrs.yml.sample para mgrs.yml

  cp /usr/share/ceph-ansible/group_vars/mgrs.yml.sample  /usr/share/ceph-ansible/group_vars/mgrs.yml

2. Entre no arquivo 'mgrs.yml' e faça alteração da linha da seguinte forma:

  copy_admin_key: true

3. Entre no inventario e descomente as linhas relacionada ao grupo MGRS e mantenha as linhas do MONS descomentadas também.

 [mons]
 ceph01.labs.corp
 ceph02.labs.corp
 ceph03.labs.corp

 [mgrs]
 ceph01.labs.corp
 ceph02.labs.corp
 ceph03.labs.corp

4. Execute o deployment do Ceph. Desta vez iremos instalar apenas o componente MGRS

 ansible-playbook -i /etc/ansible/inventario-ceph.ini -l mgrs site.yml

* Tenha certeza de estar dentro do diretorio  _/usr/share/ceph-ansible_ antes de executar o *deployment* do MGRS.

5. Após a execução do deployment do MGRS, você terá a seguinte saída:

------
 TASK [show ceph status for cluster ceph] *****************************************************************************************************************************************
 Tuesday 11 June 2019  18:25:56 -0300 (0:00:00.724)       0:05:24.107 **********
 ok: [ceph01.labs.corp -> ceph01.labs.corp] => {
    "msg": [
        "  cluster:",
        "    id:     4c595cd7-220d-48d7-80fc-88f4b6dd8a84", <1>
        "    health: HEALTH_OK",
        " ",
        "  services:",
        "    mon: 3 daemons, quorum ceph01,ceph02,ceph03",
        "    mgr: ceph01(active), standbys: ceph02, ceph03", <1>
        "    osd: 0 osds: 0 up, 0 in",<3>
        " ",
        "  data:",
        "    pools:   0 pools, 0 pgs",
        "    objects: 0 objects, 0B",
        "    usage:   0B used, 0B / 0B avail",
        "    pgs:     ",
        " "
    ]
}

PLAY RECAP ***********************************************************************************************************************************************************************
ceph01.labs.corp           : ok=153  changed=7    unreachable=0    failed=0
ceph02.labs.corp           : ok=139  changed=7    unreachable=0    failed=0
ceph03.labs.corp           : ok=141  changed=10   unreachable=0    failed=0


INSTALLER STATUS *****************************************************************************************************************************************************************
Install Ceph Monitor        : Complete (0:01:39)
Install Ceph Manager        : Complete (0:03:16)

Tuesday 11 June 2019  18:25:56 -0300 (0:00:00.066)       0:05:24.173 **********
===============================================================================
ceph-mgr : install ceph-mgr package on RedHat or SUSE ------------------------------------------------------------------------------------------------------------------- 105.90s
ceph-common : install redhat dependencies -------------------------------------------------------------------------------------------------------------------------------- 18.23s
ceph-common : install redhat ceph packages ------------------------------------------------------------------------------------------------------------------------------- 17.12s
ceph-common : install redhat ceph packages ------------------------------------------------------------------------------------------------------------------------------- 16.99s
ceph-common : install redhat dependencies -------------------------------------------------------------------------------------------------------------------------------- 16.95s
ceph-common : purge yum cache --------------------------------------------------------------------------------------------------------------------------------------------- 5.92s
ceph-common : purge yum cache --------------------------------------------------------------------------------------------------------------------------------------------- 5.89s
ceph-common : check if the red hat storage monitor repo is already present ------------------------------------------------------------------------------------------------ 3.72s
ceph-config : create ceph initial directories ----------------------------------------------------------------------------------------------------------------------------- 3.69s
ceph-config : create ceph initial directories ----------------------------------------------------------------------------------------------------------------------------- 3.52s
ceph-common : check if the red hat storage monitor repo is already present ------------------------------------------------------------------------------------------------ 3.51s
ceph-mon : create ceph mgr keyring(s) when mon is not containerized ------------------------------------------------------------------------------------------------------- 3.47s
ceph-mgr : disable ceph mgr enabled modules ------------------------------------------------------------------------------------------------------------------------------- 3.25s
ceph-config : generate ceph configuration file: ceph.conf ----------------------------------------------------------------------------------------------------------------- 2.98s
ceph-config : generate ceph configuration file: ceph.conf ----------------------------------------------------------------------------------------------------------------- 2.85s
gather and delegate facts ------------------------------------------------------------------------------------------------------------------------------------------------- 2.56s
ceph-mon : copy keys to the ansible server -------------------------------------------------------------------------------------------------------------------------------- 2.40s
ceph-infra : open monitor and manager ports ------------------------------------------------------------------------------------------------------------------------------- 1.92s
ceph-mgr : copy ceph keyring(s) if needed --------------------------------------------------------------------------------------------------------------------------------- 1.85s
ceph-mon : set keys permissions ------------------------------------------------------------------------------------------------------------------------------------------- 1.28s
[ansible@cephdeploy ceph-ansible]$
------

<1> O ID do cluster não foi alterado
<2> O serivço de MGR foi ativado em três nós (ceph01 -> ceph03). O serviço está sendo divido junto o MON no mesmo servidor
<3> Repare nosso OSD ainda não foi instalado

=== Atividade 3.3 : Utilizando Ceph-ansible para instalação do  OSD

Já temos toda a parte de monitoramento MON e MGR implementada. Agora vamos instalar a segunda parte da cluster Ceph. Que é parte do seu armazenamento OSD.


1. Altere o arquivo osds.yml.sample para osd.yml

  cp /usr/share/ceph-ansible/group_vars/osds.yml.sample  /usr/share/ceph-ansible/group_vars/osds.yml

2. Entre no inventario e descomente as linhas relacionada ao grupo OSD - Saida esperada:

 [mons]
 ceph01.labs.corp
 ceph02.labs.corp
 ceph03.labs.corp

 [mgrs]
 ceph01.labs.corp
 ceph02.labs.corp
 ceph03.labs.corp

 [osds]
 ceph04.labs.corp
 ceph05.labs.corp
 ceph06.labs.corp

3. Verifique os discos adicionais estão instalados no servidores OSD

  ansible -i /etc/ansible/inventario-ceph.ini  osds -m shell -a "lsblk"

  ceph06.labs.corp | SUCCESS | rc=0 >>
  NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
  fd0             2:0    1    4K  0 disk
  sr0            11:0    1 1024M  0 rom
  vda           252:0    0   60G  0 disk
  |-vda1        252:1    0    1G  0 part /boot
  `-vda2        252:2    0   59G  0 part
    |-rhel-root 253:0    0   51G  0 lvm  /
    `-rhel-swap 253:1    0    8G  0 lvm  [SWAP]
  vdb           252:16   0   15G  0 disk <1>
  vdc           252:32   0   15G  0 disk <2>
  vdd           252:48   0   15G  0 disk <3>

  ceph05.labs.corp | SUCCESS | rc=0 >>
  NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
  fd0             2:0    1    4K  0 disk
  sr0            11:0    1 1024M  0 rom
  vda           252:0    0   60G  0 disk
  |-vda1        252:1    0    1G  0 part /boot
  `-vda2        252:2    0   59G  0 part
    |-rhel-root 253:0    0   51G  0 lvm  /
    `-rhel-swap 253:1    0    8G  0 lvm  [SWAP]
  vdb           252:16   0   15G  0 disk
  vdc           252:32   0   15G  0 disk
  vdd           252:48   0   15G  0 disk

  ceph04.labs.corp | SUCCESS | rc=0 >>
  NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
  fd0             2:0    1    4K  0 disk
  sr0            11:0    1 1024M  0 rom
  vda           252:0    0   60G  0 disk
  |-vda1        252:1    0    1G  0 part /boot
  `-vda2        252:2    0   59G  0 part
    |-rhel-root 253:0    0   51G  0 lvm  /
    `-rhel-swap 253:1    0    8G  0 lvm  [SWAP]
  vdb           252:16   0   15G  0 disk
  vdc           252:32   0   15G  0 disk
  vdd           252:48   0   15G  0 disk

<1> Disco dedicado ao OSD
<2> Anote o nome destes discos para utilizarmos na configuração do OSD
<3> Repare que todos os servidores OSD tem cada um três discos, totalizando nove discos ou 9 OSD

[%hardbreaks]


4. Entre no arquivo 'osd.yml' e faça alteração da linha da seguinte forma:


  copy_admin_key: true
  devices:
   - /dev/vdb
   - /dev/vdc
  osd_scenario: collocated

* Vamos configurar apenas os discos VDB e VDC - Não configure o disco *VDD*

5. Altere o arquivo de configuração site.yml para que ele fica da seguinte forma:

------

  - hosts:
  - mons
  #  - agents
  - osds
  #  - mdss
  #  - rgws
  #  - nfss
  #  - restapis
  #  - rbdmirrors
  #  - clients
  - mgrs
  #  - iscsigws
  #  - iscsi-gws # for backward compatibility only!

------

6. Execute o deployment do Ceph. Desta vez iremos instalar apenas o componente MGRS

 ansible-playbook -i /etc/ansible/inventario-ceph.ini -l osds site.yml

* Tenha certeza de estar dentro do diretorio  _/usr/share/ceph-ansible_ antes de executar o *deployment* do OSD.


7. Cheque se a saida do seu deployment foi com sucesso. Exemplo de saída do deployment

------
 PLAY RECAP ***********************************************************************************************************************************************************************
 ceph04.labs.corp           : ok=119  changed=12   unreachable=0    failed=0
 ceph05.labs.corp           : ok=111  changed=12   unreachable=0    failed=0
 ceph06.labs.corp           : ok=111  changed=12   unreachable=0    failed=0
 INSTALLER STATUS *****************************************************************************************************************************************************************
 Install Ceph OSD            : Complete (0:05:54)

 Tuesday 11 June 2019  18:56:44 -0300 (0:00:00.106)       0:06:27.634 **********
 ===============================================================================
 ceph-common : install redhat ceph packages ------------------------------------------------------------------------------------------------------------------------------ 210.72s
 ceph-common : install redhat dependencies -------------------------------------------------------------------------------------------------------------------------------- 17.28s
 ceph-config : generate ceph configuration file: ceph.conf ---------------------------------------------------------------------------------------------------------------- 14.08s
 ceph-osd : install dependencies ------------------------------------------------------------------------------------------------------------------------------------------ 13.29s
 ceph-osd : manually prepare ceph "bluestore" non-containerized osd disk(s) with collocated osd data and journal ---------------------------------------------------------- 13.05s
 ceph-osd : activate osd(s) when device is a disk ------------------------------------------------------------------------------------------------------------------------- 11.22s
 ceph-common : purge yum cache --------------------------------------------------------------------------------------------------------------------------------------------- 6.05s
 gather and delegate facts ------------------------------------------------------------------------------------------------------------------------------------------------- 4.79s
 ceph-config : create ceph initial directories ----------------------------------------------------------------------------------------------------------------------------- 3.71s
 ceph-common : check if the red hat storage osd repo is already present ---------------------------------------------------------------------------------------------------- 3.61s
 ceph-osd : apply operating system tuning ---------------------------------------------------------------------------------------------------------------------------------- 2.29s
 ceph-osd : copy ceph key(s) if needed ------------------------------------------------------------------------------------------------------------------------------------- 1.97s
 ceph-osd : systemd start osd ---------------------------------------------------------------------------------------------------------------------------------------------- 1.85s
 ceph-infra : open osd ports ----------------------------------------------------------------------------------------------------------------------------------------------- 1.84s
 ceph-infra : restart firewalld -------------------------------------------------------------------------------------------------------------------------------------------- 1.81s
 ceph-facts : is ceph running already? ------------------------------------------------------------------------------------------------------------------------------------- 1.71s
 ceph-infra : start firewalld ---------------------------------------------------------------------------------------------------------------------------------------------- 1.52s
 ceph-handler : copy osd restart script ------------------------------------------------------------------------------------------------------------------------------------ 1.15s
 ceph-validate : validate devices is actually a device --------------------------------------------------------------------------------------------------------------------- 1.12s
 ceph-osd : create gpt disk label ------------------------------------------------------------------------------------------------------------------------------------------ 0.90s
------

O deployment do OSD foi feito com sucesso. Parabéns !!!


Agora vamos checar se todos os serviços estão operando de forma correta. Loge no servidor ceph01

  ssh ansible@ceph01.labs.corp
  su -

Execute o comando para checar a saúde do Ceph cluster

  ceph -s

A saída projetada do comando:

 [root@ceph01 ~]# ceph -s   <1>
 cluster:
   id:     4c595cd7-220d-48d7-80fc-88f4b6dd8a84
   health: HEALTH_OK <2>
 services:
   mon: 3 daemons, quorum ceph01,ceph02,ceph03
   mgr: ceph01(active), standbys: ceph02, ceph03
   osd: 6 osds: 6 up, 6 in <3>

 data:
   pools:   0 pools, 0 pgs <4>
   objects: 0 objects, 0B <5>
   usage:   6.01GiB used, 83.4GiB / 89.4GiB avail <6>
   pgs: <7>

<1> O comando 'ceph -s' mostra o status do cluster
<2> Este campo demonstra o estado de saúde do cluster Ceph - Que no caso está saudável !! Está 100% !!
<3> Demonstra quantidade de OSD e quantidade OSD ativos. São três OSD nodes onde usamos dois discos em cada servidor. Totalizando 6 OSDs
<4> Este campo demonstra o pool. Ainda não montamos nenhum !! Então está certo !!
<5> Este campo demonstra o total de objetos.
<6> Está saída demontra três valores importante: Espaço utilizado para construi a infraestrutura de armazenamento - Espaço total livre - Espaço total de todos OSDs
<7> Total de PG em uso. Zero não criamos nenhum pool então Zero PGs

[%hardbreaks]

=== Atividade 3.4 : Utilizando Ceph-ansible para instalação do  ceph-client

Você percebeu que para checar o status do cluster Ceph, foi necessário entrar no servidor *ceph01.labs.corp*. Isto não é muito legal no ambienten de produção ?

Ninguém quer entrar no ambiente de produção e ficar executando comando direto na console ou dar acesso de baixo ou alto privilégio diretamente no cluster. Não fica legal.

O ideal é utlizarmos um cliente. Pode ser uma simples VM. No nosso lab foi criado o servidor *cephcliente.labs.corp* .

Vamos instalar no servidor cephcliente os componentes necessários para acessar o cluster ceph de forma segura e correta.


1. Altere o arquivo clients.yml.sample para clients.yml

  cp /usr/share/ceph-ansible/group_vars/clients.yml.sample  /usr/share/ceph-ansible/group_vars/clients.yml

2. Entre no inventario '/etc/ansible/inventario-ceph.ini' e descomente as linhas relacionada ao grupo CLIENTS - Configuração esperada:

 [mons]
 ceph01.labs.corp
 ceph02.labs.corp
 ceph03.labs.corp
 [mgrs]
 ceph01.labs.corp
 ceph02.labs.corp
 ceph03.labs.corp
 [osds]
 ceph04.labs.corp
 ceph05.labs.corp
 ceph06.labs.corp
 #[radosgws]
 #ceph07.labs.corp
 #[iscsis]
 #ceph08.labs.corp
 [clients]
 cephclient.labs.corp

3. Altere o arquivo '/usr/share/ceph-ansible/group_vars/clients.yml' da seguinte forma:

  copy_admin_key: true

4. Execute o deployment do Ceph. Desta vez iremos instalar apenas o componente CLIENT

 ansible-playbook -i /etc/ansible/inventario-ceph.ini -l clients site.yml

* Tenha certeza de estar dentro do diretorio  _/usr/share/ceph-ansible_ antes de executar o *deployment* do CLIENT.

5. Cheque se a saida do seu deployment foi com sucesso. Exemplo de saída do deployment


  PLAY RECAP ***********************************************************************************************************************************************************************
  cephclient.labs.corp       : ok=58   changed=5    unreachable=0    failed=0


  INSTALLER STATUS *****************************************************************************************************************************************************************
  Install Ceph Client         : Complete (0:05:15)

  Tuesday 11 June 2019  19:33:41 -0300 (0:00:00.077)       0:05:15.877 **********
  ===============================================================================
  ceph-common : install redhat ceph packages ------------------------------------------------------------------------------------------------------------------------------ 270.64s
  ceph-common : install redhat dependencies -------------------------------------------------------------------------------------------------------------------------------- 16.44s
  ceph-common : purge yum cache --------------------------------------------------------------------------------------------------------------------------------------------- 5.44s
  ceph-config : generate ceph configuration file: ceph.conf ----------------------------------------------------------------------------------------------------------------- 3.73s
  ceph-common : check if the red hat storage tools repo is already present -------------------------------------------------------------------------------------------------- 3.18s
  ceph-config : create ceph initial directories ----------------------------------------------------------------------------------------------------------------------------- 2.80s
  ceph-facts : is ceph running already? ------------------------------------------------------------------------------------------------------------------------------------- 1.19s
  ceph-facts : check if it is atomic host ----------------------------------------------------------------------------------------------------------------------------------- 0.80s
  ceph-client : copy ceph admin keyring ------------------------------------------------------------------------------------------------------------------------------------- 0.69s
  ceph-common : configure cluster name -------------------------------------------------------------------------------------------------------------------------------------- 0.46s
  ceph-common : get ceph version -------------------------------------------------------------------------------------------------------------------------------------------- 0.45s
  ceph-facts : create a local fetch directory if it does not exist ---------------------------------------------------------------------------------------------------------- 0.40s
  ceph-config : create ceph conf directory ---------------------------------------------------------------------------------------------------------------------------------- 0.35s
  ceph-facts : check if the ceph conf exists -------------------------------------------------------------------------------------------------------------------------------- 0.27s
  ceph-facts : check if ~/ceph-ansible-keys directory exists ---------------------------------------------------------------------------------------------------------------- 0.23s
  ceph-facts : reuse cluster fsid when cluster is already running ----------------------------------------------------------------------------------------------------------- 0.21s
  ceph-facts : read cluster fsid if it already exists ----------------------------------------------------------------------------------------------------------------------- 0.21s
  ceph-handler : restart ceph mgr daemon(s) - non container ----------------------------------------------------------------------------------------------------------------- 0.16s
  ceph-config : set_fact ceph_directories ----------------------------------------------------------------------------------------------------------------------------------- 0.13s
  ceph-handler : set _rgw_handler_called before restart --------------------------------------------------------------------------------------------------------------------- 0.13s

 6. Acessando o cluster Ceph via Ceph client

Uma vez com   _cephclient_ instalado poderemos acessar o cluster Ceph, fora dos nós do cluster.

  ssh ansible@cephclient
  Last login: Tue Jun 11 14:28:28 2019 from 192.168.10.50
  [ansible@cephclient ~]$ su -
  Password:
  Last login: Mon Jun 10 18:01:54 -03 2019 from 192.168.10.50 on pts/0
  [root@cephclient ~]# ceph -s
    cluster:
      id:     4c595cd7-220d-48d7-80fc-88f4b6dd8a84
      health: HEALTH_OK
   services:
      mon: 3 daemons, quorum ceph01,ceph02,ceph03
      mgr: ceph01(active), standbys: ceph02, ceph03
      osd: 6 osds: 6 up, 6 in
    data:
      pools:   0 pools, 0 pgs
      objects: 0 objects, 0B
      usage:   6.01GiB used, 83.4GiB / 89.4GiB avail
      pgs:

[%hardbreaks]

*Deu erro na instalação ?*

Não tem problema, o ceph-ansible permite você destruir todo o cluster e começar de novo.

Basta executar o playbook *purge.yml*

 [ansible@cephdeploy ceph-ansible]$ ansible-playbook -i /etc/ansible/inventario-ceph.ini infrastructure-playbooks/purge-cluster.yml
 [WARNING]: log file at /var/log/ansible.log is not writeable and we cannot create it, aborting
 Are you sure you want to purge the cluster? [no]: yes

------


 Tuesday 11 June 2019  17:46:12 -0300 (0:00:00.194)       0:03:26.556 **********
 ===============================================================================
 purge remaining ceph packages with yum ----------------------------------------------------------------------------------------------------------------------------------- 74.13s
 purge ceph packages with yum --------------------------------------------------------------------------------------------------------------------------------------------- 70.21s
 Gathering Facts ----------------------------------------------------------------------------------------------------------------------------------------------------------- 2.30s
 remove monitor store and bootstrap keys ----------------------------------------------------------------------------------------------------------------------------------- 2.28s
 request data removal ------------------------------------------------------------------------------------------------------------------------------------------------------ 1.76s
 get ceph lockbox partitions ----------------------------------------------------------------------------------------------------------------------------------------------- 1.59s
 drop all cache ------------------------------------------------------------------------------------------------------------------------------------------------------------ 1.58s
 stop ceph mgrs with systemd ----------------------------------------------------------------------------------------------------------------------------------------------- 1.29s
 stop ceph mons with systemd ----------------------------------------------------------------------------------------------------------------------------------------------- 1.23s
 get osd data and lockbox mount points ------------------------------------------------------------------------------------------------------------------------------------- 1.20s
 find ceph systemd unit files to remove ------------------------------------------------------------------------------------------------------------------------------------ 1.02s
 check for anything running ceph ------------------------------------------------------------------------------------------------------------------------------------------- 0.98s
 purge rh_storage.repo file in /etc/yum.repos.d ---------------------------------------------------------------------------------------------------------------------------- 0.97s
 remove ceph udev rules ---------------------------------------------------------------------------------------------------------------------------------------------------- 0.97s
 remove config ------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.96s
 remove data --------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.94s
 remove logs --------------------------------------------------------------------------------------------------------------------------------------------------------------- 0.93s
 purge rpm cache in /tmp --------------------------------------------------------------------------------------------------------------------------------------------------- 0.92s
 purge remaining ceph packages with dnf ------------------------------------------------------------------------------------------------------------------------------------ 0.79s
 get ceph block partitions ------------------------------------------------------------------------------------------------------------------------------------------------- 0.72s
 [ansible@cephdeploy ceph-ansible]$

------


=== Atividade 04: _Checar a saúde do cluster pós instalação_

Após a conclusão da instalação, você pode testar seu novo cluster do Ceph para verificar se a implementação está correta e se o cluster está funcionando.

|===
|Operação | Comando
|Show cluster status |ceph -s
|Watch live cluster changes | ceph -w
|Show cluster free space status |ceph df
|Show OSD usage | ceph osd df
|List users |ceph auth list
|Display user permissions |ceph auth caps
|Check health CEPH| ceph health
|===

Agora que você tem um arsenal de novos comandos para checar a saúde do cluster Ceph. Vá em frente e teste !!!
